{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Chance that it rains more than n (e.g. 100) days in Vancouver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Monte-Carlo simulation\n",
    "def prob_rain_more_than_n(p: Sequence[float], n: int) -> float:\n",
    "    total_days = len(p)\n",
    "    trials = 10000\n",
    "    \n",
    "    # Run multiple simulations\n",
    "    simulations = np.random.rand(trials, total_days) < p  # Generate matrix with random weather of [size trials X total_days]\n",
    "    rainy_days_per_trial = np.sum(simulations, axis=1) # Count rainy days per trial\n",
    "    \n",
    "    # Count how many trials had more than n rainy days\n",
    "    successful_trials = np.sum(rainy_days_per_trial > n)\n",
    "    \n",
    "    # Return the fraction of successful trials as the probability\n",
    "    return successful_trials / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 - Probability of more than 100 rainy days: 0.842900\n",
      "Test Case 2 - Probability of more than 180 rainy days: 0.636600\n",
      "Test Case 3 - Probability of more than 100 rainy days: 0.885400\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a list of probabilities between 0.1 and 0.5\n",
    "def generate_probabilities(min_value: float, max_value: float, days: int) -> List[float]:\n",
    "    return [random.uniform(min_value, max_value) for _ in range(days)]\n",
    "\n",
    "# Test case 1: Probability of rain is 0.3 on each day\n",
    "p_1 = [0.3] * 365\n",
    "n_1 = 100\n",
    "result_1 = prob_rain_more_than_n(p_1, n_1)\n",
    "print(f\"Test Case 1 - Probability of more than {n_1} rainy days: {result_1:.6f}\")\n",
    "\n",
    "# Test case 2: Probability of rain varies between 0.1 and 0.9\n",
    "p_2 = [0.1, 0.9] * (365 // 2) + [0.5]  # alternating 0.1 and 0.9, with last day 0.5\n",
    "n_2 = 180\n",
    "result_2 = prob_rain_more_than_n(p_2, n_2)\n",
    "print(f\"Test Case 2 - Probability of more than {n_2} rainy days: {result_2:.6f}\")\n",
    "\n",
    "# Test case 3: Probability of rain varies between 0.1 and 0.5\n",
    "p_3 = generate_probabilities(0.1, 0.5, 365)  # 365 probabilities between 0.1 and 0.5\n",
    "n_3 = 100\n",
    "result_3 = prob_rain_more_than_n(p_3, n_3)\n",
    "print(f\"Test Case 3 - Probability of more than {n_3} rainy days: {result_3:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: All combinations of the words that can produce a word sequence from the given sequence of phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pronunciation dictionary\n",
    "pronunciation_dict = {\n",
    "    \"ABACUS\": [\"AE\", \"B\", \"AH\", \"K\", \"AH\", \"S\"],\n",
    "    \"BOOK\": [\"B\", \"UH\", \"K\"],\n",
    "    \"THEIR\": [\"DH\", \"EH\", \"R\"],\n",
    "    \"THERE\": [\"DH\", \"EH\", \"R\"],\n",
    "    \"TOMATO_1\": [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\"],\n",
    "    \"TOMATO_2\": [\"T\", \"AH\", \"M\", \"EY\", \"T\", \"OW\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dictionary to create a phoneme-to-words map\n",
    "def preprocess_pronunciation_dict(pronunciation_dict):\n",
    "    phoneme_to_words = defaultdict(list)\n",
    "    for word, phonemes in pronunciation_dict.items():\n",
    "        phoneme_tuple = tuple(phonemes)\n",
    "        phoneme_to_words[phoneme_tuple].append(word)\n",
    "    return phoneme_to_words\n",
    "\n",
    "# Function to find all combinations of words with the given phoneme sequence\n",
    "def find_word_combos_with_pronunciation(phonemes: Sequence[str]) -> List[Sequence[str]]:\n",
    "    phoneme_to_words = preprocess_pronunciation_dict(pronunciation_dict)\n",
    "    \n",
    "    # Recursive function to find combinations\n",
    "    def find_combinations(start):\n",
    "        if start == len(phonemes):\n",
    "            return [[]]  # If we reach the end, return an empty list as the base case\n",
    "        results = []\n",
    "        for length in range(1, len(phonemes) - start + 1):\n",
    "            sub_phonemes = tuple(phonemes[start:start + length])\n",
    "            if sub_phonemes in phoneme_to_words:\n",
    "                for word in phoneme_to_words[sub_phonemes]:\n",
    "                    for rest in find_combinations(start + length):\n",
    "                        results.append([word] + rest)\n",
    "        return results\n",
    "    \n",
    "    return find_combinations(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THEIR', 'THEIR']\n",
      "['THEIR', 'THERE']\n",
      "['THERE', 'THEIR']\n",
      "['THERE', 'THERE']\n"
     ]
    }
   ],
   "source": [
    "# Example usage: exact match\n",
    "phonemes_1 = [\"DH\", \"EH\", \"R\", \"DH\", \"EH\", \"R\"]\n",
    "combinations_1 = find_word_combos_with_pronunciation(phonemes_1)\n",
    "for combo in combinations_1:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No words match the phoneme given !\n"
     ]
    }
   ],
   "source": [
    "# Example usage: no match\n",
    "phonemes_2 = [\"X\", \"Y\", \"Z\"]\n",
    "combinations_2 = find_word_combos_with_pronunciation(phonemes_2)\n",
    "\n",
    "if combinations_2:\n",
    "    for combo in combinations_2:\n",
    "        print(combo)\n",
    "else:\n",
    "    print(\"No words match the phoneme given !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOMATO_1', 'TOMATO_2']\n"
     ]
    }
   ],
   "source": [
    "# Example usage: exact match\n",
    "phonemes_3 = [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\", \"T\", \"AH\", \"M\", \"EY\", \"T\", \"OW\"]\n",
    "combinations_3 = find_word_combos_with_pronunciation(phonemes_3)\n",
    "\n",
    "if combinations_3:\n",
    "    for combo in combinations_3:\n",
    "        print(combo)\n",
    "else:\n",
    "    print(\"No words match the phoneme given !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: CTC implementation according to the paper titled \"CTCModel: a Keras Model for Connectionist Temporal Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for CTC loss\n",
    "def ctc_loss_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "# Helper function for CTC decoding\n",
    "def ctc_decode_lambda_func(args, greedy=True, beam_width=100, top_paths=1):\n",
    "    y_pred, input_length = args\n",
    "    decoded, _ = K.ctc_decode(y_pred, input_length, greedy=greedy, beam_width=beam_width, top_paths=top_paths)\n",
    "    return [tf.sparse.to_dense(seq) for seq in decoded]\n",
    "\n",
    "# Helper function for LER and SER calculation\n",
    "def ctc_metrics_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    \n",
    "    # Decode predictions with CTC decoding\n",
    "    decoded, _ = K.ctc_decode(y_pred, input_length, greedy=True)\n",
    "    \n",
    "    # Convert labels and predictions to sparse tensors for edit distance calculation\n",
    "    label_sparse = tf.cast(tf.keras.backend.ctc_label_dense_to_sparse(labels, label_length), dtype=tf.int64)\n",
    "    pred_sparse = tf.cast(decoded[0], dtype=tf.int64)\n",
    "    \n",
    "    # Calculate edit distance for LER and SER\n",
    "    edit_distances = tf.edit_distance(pred_sparse, label_sparse, normalize=True)\n",
    "    ler = tf.reduce_mean(edit_distances)  # Label Error Rate (LER)\n",
    "    ser = tf.reduce_mean(tf.cast(edit_distances > 0, tf.float32))  # Sequence Error Rate (SER)\n",
    "    \n",
    "    # Return LER and SER as a list of tensors\n",
    "    return [ler, ser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCModel:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.model_train = None\n",
    "        self.model_predict = None\n",
    "        self.model_eval = None\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.build_model()\n",
    "\n",
    "    def get_model_train(self):\n",
    "        # Model used for training\n",
    "        return self.model_train\n",
    "\n",
    "    def get_model_predict(self):\n",
    "        # Model used for testing\n",
    "        return self.model_predict\n",
    "\n",
    "    def get_model_eval(self):\n",
    "        # Model used for evaluating\n",
    "        return self.model_eval\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for the images (or sequences)\n",
    "        self.inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "\n",
    "        # Add layers: for instance, BiLSTM for recurrent neural networks\n",
    "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(self.inputs)\n",
    "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "\n",
    "        # Output softmax layer (num_classes + 1 for the blank token)\n",
    "        self.outputs = layers.Dense(self.num_classes + 1, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Inputs for CTC loss: true labels, input lengths, and label lengths\n",
    "        labels = layers.Input(shape=(None,), dtype='int32', name='labels')\n",
    "        input_length = layers.Input(shape=(1,), dtype='int32', name='input_length')\n",
    "        label_length = layers.Input(shape=(1,), dtype='int32', name='label_length')\n",
    "\n",
    "        # Compute the CTC loss using a Lambda layer\n",
    "        ctc_loss = layers.Lambda(ctc_loss_lambda_func, output_shape=(1,), name='ctc_loss')(\n",
    "            [self.outputs, labels, input_length, label_length])\n",
    "\n",
    "        # Training model: takes inputs, labels, input lengths, and label lengths\n",
    "        self.model_train = Model(inputs=[self.inputs, labels, input_length, label_length], outputs=ctc_loss)\n",
    "\n",
    "        # Prediction model: used to predict sequences\n",
    "        self.model_predict = Model(inputs=self.inputs, outputs=self.outputs)\n",
    "\n",
    "        # Evaluation model: calculates LER and SER\n",
    "        ler_ser = layers.Lambda(ctc_metrics_lambda_func, name=\"ctc_metrics\")(\n",
    "            [self.outputs, labels, input_length, label_length])\n",
    "        self.model_evaluate = Model(inputs=[self.inputs, labels, input_length, label_length], outputs=ler_ser)\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        self.model_train.compile(optimizer=optimizer, loss={'ctc_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "    def train(self, x, y, input_lengths, label_lengths, batch_size=32, epochs=10):\n",
    "        self.model_train.fit([x, y, input_lengths, label_lengths], y=None, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    def predict(self, x, input_lengths, greedy=True, beam_width=100, top_paths=1):\n",
    "        # Get raw predictions from the model\n",
    "        y_pred = self.model_predict.predict(x)\n",
    "        \n",
    "        # Decode predictions with CTC decoding\n",
    "        decoded_sequences = ctc_decode_lambda_func([y_pred, input_lengths], greedy=greedy, beam_width=beam_width, top_paths=top_paths)\n",
    "        \n",
    "        return decoded_sequences\n",
    "\n",
    "    def evaluate(self, x, y, input_lengths, label_lengths):\n",
    "        # Evaluate the model on some dataset, calculating LER and SER\n",
    "        ler, ser = self.model_evaluate.predict([x, y, input_lengths, label_lengths])\n",
    "        print(f\"Label Error Rate (LER): {ler}, Sequence Error Rate (SER): {ser}\")\n",
    "        return ler, ser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional question\n",
    "### Q3: N most common words in Shakespeare dataset\n",
    "#### I implemented this in Python since I'm more comfortable with Python as compared to C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare text file\n",
    "file_path = \"data/shakespeare.txt\"\n",
    "\n",
    "# Initialize a Counter for word frequencies\n",
    "word_counts = Counter()\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # Convert line to lowercase\n",
    "        line = line.lower()\n",
    "        \n",
    "        # Remove special characters and split into words\n",
    "        words = re.findall(r'\\b\\w+\\b', line)\n",
    "        \n",
    "        # Update word counts\n",
    "        word_counts.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words:\n",
      "the: 6287\n",
      "and: 5690\n",
      "i: 5111\n",
      "to: 4934\n",
      "of: 3760\n",
      "you: 3211\n",
      "my: 3120\n",
      "a: 3018\n",
      "that: 2664\n",
      "in: 2403\n"
     ]
    }
   ],
   "source": [
    "# Example usage: 10 most common words\n",
    "n = 10\n",
    "\n",
    "most_common_words = word_counts.most_common(n)\n",
    "\n",
    "print(\"Most frequent words:\")\n",
    "for word, freq in most_common_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
