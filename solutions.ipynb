{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Sequence\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo simulation\n",
    "def prob_rain_more_than_n(p: Sequence[float], n: int) -> float:\n",
    "    total_days = len(p)\n",
    "    trials = 10000\n",
    "    \n",
    "    # Run multiple simulations\n",
    "    simulations = np.random.rand(trials, total_days) < p  # Generate matrix with random weather of [size trials X total_days]\n",
    "    rainy_days_per_trial = np.sum(simulations, axis=1) # Count rainy days per trial\n",
    "    \n",
    "    # Count how many trials had more than n rainy days\n",
    "    successful_trials = np.sum(rainy_days_per_trial > n)\n",
    "    \n",
    "    # Return the fraction of successful trials as the probability\n",
    "    return successful_trials / trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 - Probability of more than 100 rainy days: 0.852900\n",
      "Test Case 2 - Probability of more than 180 rainy days: 0.636900\n",
      "Test Case 3 - Probability of more than 100 rainy days: 0.892600\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a list of probabilities between 0.1 and 0.5\n",
    "def generate_probabilities(min_value: float, max_value: float, days: int) -> List[float]:\n",
    "    return [random.uniform(min_value, max_value) for _ in range(days)]\n",
    "\n",
    "# Test case 1: Probability of rain is 0.3 on each day\n",
    "p_1 = [0.3] * 365\n",
    "n_1 = 100\n",
    "result_1 = prob_rain_more_than_n(p_1, n_1)\n",
    "print(f\"Test Case 1 - Probability of more than {n_1} rainy days: {result_1:.6f}\")\n",
    "\n",
    "# Test case 2: Probability of rain varies between 0.1 and 0.9\n",
    "p_2 = [0.1, 0.9] * (365 // 2) + [0.5]  # alternating 0.1 and 0.9, with last day 0.5\n",
    "n_2 = 180\n",
    "result_2 = prob_rain_more_than_n(p_2, n_2)\n",
    "print(f\"Test Case 2 - Probability of more than {n_2} rainy days: {result_2:.6f}\")\n",
    "\n",
    "# Test case 3: Probability of rain varies between 0.1 and 0.5\n",
    "p_3 = generate_probabilities(0.1, 0.5, 365)  # 365 probabilities between 0.1 and 0.5\n",
    "n_3 = 100\n",
    "result_3 = prob_rain_more_than_n(p_3, n_3)\n",
    "print(f\"Test Case 3 - Probability of more than {n_3} rainy days: {result_3:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pronunciation dictionary\n",
    "pronunciation_dict = {\n",
    "    \"ABACUS\": [\"AE\", \"B\", \"AH\", \"K\", \"AH\", \"S\"],\n",
    "    \"BOOK\": [\"B\", \"UH\", \"K\"],\n",
    "    \"THEIR\": [\"DH\", \"EH\", \"R\"],\n",
    "    \"THERE\": [\"DH\", \"EH\", \"R\"],\n",
    "    \"TOMATO_1\": [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\"],\n",
    "    \"TOMATO_2\": [\"T\", \"AH\", \"M\", \"EY\", \"T\", \"OW\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dictionary to create a phoneme-to-words map\n",
    "def preprocess_pronunciation_dict(pronunciation_dict):\n",
    "    phoneme_to_words = defaultdict(list)\n",
    "    for word, phonemes in pronunciation_dict.items():\n",
    "        phoneme_tuple = tuple(phonemes)\n",
    "        phoneme_to_words[phoneme_tuple].append(word)\n",
    "    return phoneme_to_words\n",
    "\n",
    "# Function to find all combinations of words with the given phoneme sequence\n",
    "def find_word_combos_with_pronunciation(phonemes: Sequence[str]) -> List[Sequence[str]]:\n",
    "    phoneme_to_words = preprocess_pronunciation_dict(pronunciation_dict)\n",
    "    \n",
    "    # Recursive function to find combinations\n",
    "    def find_combinations(start):\n",
    "        if start == len(phonemes):\n",
    "            return [[]]  # If we reach the end, return an empty list as the base case\n",
    "        results = []\n",
    "        for length in range(1, len(phonemes) - start + 1):\n",
    "            sub_phonemes = tuple(phonemes[start:start + length])\n",
    "            if sub_phonemes in phoneme_to_words:\n",
    "                for word in phoneme_to_words[sub_phonemes]:\n",
    "                    for rest in find_combinations(start + length):\n",
    "                        results.append([word] + rest)\n",
    "        return results\n",
    "    \n",
    "    return find_combinations(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THEIR', 'THEIR']\n",
      "['THEIR', 'THERE']\n",
      "['THERE', 'THEIR']\n",
      "['THERE', 'THERE']\n"
     ]
    }
   ],
   "source": [
    "# Example usage: exact match\n",
    "phonemes_1 = [\"DH\", \"EH\", \"R\", \"DH\", \"EH\", \"R\"]\n",
    "combinations_1 = find_word_combos_with_pronunciation(phonemes_1)\n",
    "for combo in combinations_1:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No words match the phoneme given !\n"
     ]
    }
   ],
   "source": [
    "# Example usage: no match\n",
    "phonemes_2 = [\"X\", \"Y\", \"Z\"]\n",
    "combinations_2 = find_word_combos_with_pronunciation(phonemes_2)\n",
    "\n",
    "if combinations_2:\n",
    "    for combo in combinations_2:\n",
    "        print(combo)\n",
    "else:\n",
    "    print(\"No words match the phoneme given !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOMATO_1', 'TOMATO_2']\n"
     ]
    }
   ],
   "source": [
    "# Example usage: exact match\n",
    "phonemes_3 = [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\", \"T\", \"AH\", \"M\", \"EY\", \"T\", \"OW\"]\n",
    "combinations_3 = find_word_combos_with_pronunciation(phonemes_3)\n",
    "\n",
    "if combinations_3:\n",
    "    for combo in combinations_3:\n",
    "        print(combo)\n",
    "else:\n",
    "    print(\"No words match the phoneme given !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example word to phoneme data\n",
    "data = [\n",
    "    (\"THEIR\", [\"DH\", \"EH\", \"R\"]),\n",
    "    (\"THERE\", [\"DH\", \"EH\", \"R\"]),\n",
    "    (\"ABACUS\", [\"AE\", \"B\", \"AH\", \"K\", \"AH\", \"S\"]),\n",
    "    (\"TOMATO\", [\"T\", \"AH\", \"M\", \"AA\", \"T\", \"OW\"]),\n",
    "    # Add more words and phonemes\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize phonemes\n",
    "phoneme_tokenizer = Tokenizer()\n",
    "phoneme_tokenizer.fit_on_texts([item[1] for item in data])\n",
    "phoneme_sequences = phoneme_tokenizer.texts_to_sequences([item[1] for item in data])\n",
    "\n",
    "# Tokenize words\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts([item[0] for item in data])\n",
    "word_sequences = word_tokenizer.texts_to_sequences([item[0] for item in data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# # Define model parameters\n",
    "# embedding_size = 64\n",
    "# hidden_size = 128\n",
    "# num_phonemes = len(phoneme_tokenizer.word_index) + 1  # Vocabulary size for phonemes\n",
    "# num_words = len(word_tokenizer.word_index) + 1  # Vocabulary size for words\n",
    "\n",
    "# # Encoder\n",
    "# encoder_inputs = Input(shape=(None,))  # Variable length input\n",
    "# encoder_embedding = Embedding(input_dim=num_phonemes, output_dim=embedding_size)(encoder_inputs)  # Embed phoneme indices\n",
    "# encoder_lstm = LSTM(hidden_size, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Decoder\n",
    "# decoder_inputs = Input(shape=(None,))\n",
    "# decoder_embedding = Embedding(input_dim=num_words, output_dim=embedding_size)(decoder_inputs)  # Embed word indices\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# decoder_dense = Dense(num_words, activation='softmax')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Seq2Seq model\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# # Train the model\n",
    "# # Note: Ensure `phoneme_sequences` and `word_sequences` are properly tokenized and padded\n",
    "# model.fit([phoneme_sequences, word_sequences], epochs=50, batch_size=64)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
